{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db94ba08-3a90-4a44-ac56-9233e94ac3b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/carsdataflow_databricks_notebooks/setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beea8bf2-7379-46c0-896a-de40fab5e643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path = \"abfss://silver@adlsforcarsdataflow.dfs.core.windows.net/Carsdata/\"\n",
    "gold_path = \"abfss://gold@adlsforcarsdataflow.dfs.core.windows.net/Carsdata/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c7af434-9606-482c-be59-c92c53e006dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating Databases to store dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59d0fab8-64ce-4034-bff3-7699eebc8c35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We know the top most layer is Hive metastore, inside the metastore the next level object is Database and inside databases we store the table. Let's Create 2 databases and store the tables in them.\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS carsdataflow_silver\") # To store different car dimension tables\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS carsdataflow_gold\") # To store the one big Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6915965a-4b76-4345-9f46-8a6865c0ff39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading the Data from the Silver Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39ca13b5-48e4-4b95-b810-43d4a732e298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_src = spark.read.format(\"delta\").load(silver_path)\n",
    "df_src.display()\n",
    "df_src = df_src.select(\"Date_ID\", \"Day\", \"Month\", \"Year\")\n",
    "df_src.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429cc6d4-8ad6-4ff3-b8bb-0faaabead00c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SCD Type 1 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "353c02db-9ade-478b-a000-5757644b0884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating Model Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34be87c7-4184-429c-8160-251466e1171c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# To run sql queries on top of the df I am creating a temp view as Model_view\n",
    "df_src.createOrReplaceTempView(\"date_view\")\n",
    "\n",
    "date_df = spark.sql(\n",
    "    \"SELECT DISTINCT Date_ID, Day, Month, Year FROM date_view\"\n",
    ")\n",
    "display(date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "459ed1a0-263d-4d2c-8dbf-ec59eaadcadb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_src = date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b19fc3f-8621-41d1-b225-d3cd29b45ae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE DATABASE carsdataflow_silver;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004f6578-1443-4332-83a5-805a4bba7d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# initial we dont have any schema to insert for the first time the data into the table. so first we check whether we already have a table or need to create a table schema, so that it will help for the incremental load as well.\n",
    "# To identify whether we have a table or not we use the incremental parameter that will let us know whether we have a table or not.\n",
    "# if incremental status set to false that means it is initial load else table and data is incremental\n",
    "\n",
    "dbutils.widgets.text(\"incremental_status\", \"false\")\n",
    "incremental_status = dbutils.widgets.get(\"incremental_status\")  \n",
    "print(incremental_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e1d909c-c212-4c01-982a-d998ab67ad26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if incremental_status == \"false\":\n",
    "    df_sink = spark.sql(\"select 1 as Date_Key, Date_ID, Day, Month, Year from date_view where 1 = 0\")\n",
    "else:\n",
    "    df_sink = spark.sql(\"select Date_key, Date_ID, Day, Month, Year from carsdataflow_silver.dim_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5948215a-0f16-4031-b04f-bee67f04d39c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Applying Join for the df_src & df_sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9111cab2-87d2-45f0-88d0-14dd7f777d1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df_filter = df_src.join(\n",
    "    df_sink, \n",
    "    df_src.Date_ID == df_sink.Date_ID, \n",
    "    \"left\"\n",
    ").select(\n",
    "    df_sink[\"Date_Key\"], \n",
    "    df_src[\"Date_ID\"], \n",
    "    df_src[\"Day\"], \n",
    "    df_src[\"Month\"],\n",
    "    df_src[\"Year\"]\n",
    ")\n",
    "display(df_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a45eff6-05c5-4e8b-86e8-9a31cf4bb018",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Findout what are new records and old records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "263e8119-d190-487e-8263-c597d1f69675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filter_old = df_filter.filter(df_filter.Date_Key.isNotNull())\n",
    "df_filter_old.display()\n",
    "df_filter_new = df_filter.filter(df_filter.Date_Key.isNull()).select(\"Date_ID\", \"Day\", \"Month\", \"Year\") # We already know Model_key will always null for the new records so selecting only Date_ID\", \"Day\", \"Month\", \"Year\"\n",
    "df_filter_new.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dfab09f-eecc-4b04-9bef-6a775e641199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating Surrogate Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d83a4ecf-a1c2-40e8-9c64-8633c4f8858d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "if (incremental_status == \"false\"):\n",
    "    max_surrogate_key = 1\n",
    "else:\n",
    "    max_surrogate_key = spark.sql(\n",
    "        \"select max(Date_Key) as max_surrogate_key from carsdataflow_silver.dim_date\"\n",
    "    ).collect()[0][0]\n",
    "\n",
    "print(max_surrogate_key)\n",
    "\n",
    "# We are creating a new surrogate key for the new records and appending it to the new records\n",
    "df_filter_new = df_filter_new.withColumn(\n",
    "    \"Date_Key\", max_surrogate_key + monotonically_increasing_id()\n",
    ")\n",
    "\n",
    "display(df_filter_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d015dced-9ba4-4f50-abb3-36ba3eea0d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Combine both old and new filtered records and make into single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce2e3de-1a0e-48ad-b940-dc16980cbe27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = df_filter_new.union(df_filter_old)\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "191c2eb2-3dae-4415-9ec7-a79ccab216e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from delta.tables import *\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = f\"{gold_path}/dim_date\"\n",
    "\n",
    "# Deduplicate the source DataFrame\n",
    "deduplicated_final_df = final_df.dropDuplicates([\"Date_Key\"])\n",
    "\n",
    "# Ensure no duplicate keys in the source DataFrame\n",
    "deduplicated_final_df = deduplicated_final_df.dropDuplicates([\"Date_ID\"])\n",
    "\n",
    "if spark.catalog.tableExists(\"carsdataflow_silver.dim_Date\"):\n",
    "    delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "    delta_table.alias(\"t\").merge(\n",
    "        deduplicated_final_df.alias(\"s\"),\n",
    "        \"t.Date_ID = s.Date_ID\"\n",
    "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "else:\n",
    "    deduplicated_final_df.write.format(\"delta\").mode(\"overwrite\").option(\n",
    "        \"path\", delta_table_path\n",
    "    ).saveAsTable(\"carsdataflow_silver.dim_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a278aa-ae70-4eec-976c-95b2d01614a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from carsdataflow_silver.dim_date;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 162071306375276,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver-to-gold-for-dim-date",
   "widgets": {
    "incremental_status": {
     "currentValue": "false",
     "nuid": "8ab7cb4a-b245-4b23-af6b-2555ddcb1e2b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": null,
      "name": "incremental_status",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "false",
      "label": null,
      "name": "incremental_status",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
